{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPdEGt0ckVHrVRYUkR5Da0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antisaint669/scaling-octo-rffnnet/blob/main/WorkingCryptoPredictor9_1_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpITKBP0D0Fn",
        "outputId": "9b48c61c-7ca0-4b8f-a64d-6bb3d14b8216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the data:\n",
            "Index(['Date', 'End', 'Open', 'High', 'Low', 'Close', 'Volume', 'MarketCap'], dtype='object')\n",
            "Columns being used for prediction: ['High', 'Low', 'Close']\n",
            "\n",
            "Starting predictions:\n",
            "Processing High\n",
            "No saved weights for High. Training...\n"
          ]
        }
      ],
      "source": [
        "# Example of the file.csv in the /home/file.csv\n",
        "\n",
        "# Date,End,Open,High,Low,Close,Volume,MarketCap\n",
        "# 2014-09-17,465.864014,468.174011,452.421997,457.334015,457.334015,21056800\n",
        "# 2014-09-18,456.859985,456.859985,413.104004,424.440002,424.440002,34483200\n",
        "# 2014-09-19,424.102997,427.834991,384.532013,394.795990,394.795990,37919700\n",
        "# 2014-09-20,394.673004,423.295990,389.882996,408.903992,408.903992,36863600\n",
        "# 2014-09-21,408.084991,412.425995,393.181000,398.821014,398.821014,26580100\n",
        "# 2014-09-22,399.100006,406.915985,397.130005,402.152008,402.152008,24127600\n",
        "# 2014-09-23,402.092010,441.557007,396.196991,435.790985,435.790985,45099500\n",
        "# 2014-09-24,435.751007,436.112000,421.131989,423.204987,423.204987,30627700\n",
        "# 2014-09-25,423.156006,423.519989,409.467987,411.574005,411.574005,26814400\n",
        "# 2014-09-26,411.428986,414.937988,400.009003,404.424988,404.424988,21460800\n",
        "# 2014-09-27,403.556000,406.622986,397.372009,399.519989,399.519989,15029300\n",
        "# 2014-09-28,399.471008,401.016998,374.332001,377.181000,377.181000,23613300\n",
        "\n",
        "!pip install numpy pandas tensorflow scikit-learn yfinance requests\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import timedelta\n",
        "import hashlib\n",
        "\n",
        "FILE_PATH = '/home/file.csv'\n",
        "data = pd.read_csv(FILE_PATH)\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Print available columns\n",
        "print(\"Columns in the data:\")\n",
        "print(data.columns)\n",
        "\n",
        "# Define the columns we want to predict\n",
        "target_columns = ['High', 'Low', 'Close']\n",
        "\n",
        "# Filter to only use columns that are actually in the data\n",
        "available_columns = [col for col in target_columns if col in data.columns]\n",
        "\n",
        "if not available_columns:\n",
        "    raise ValueError(f\"None of the target columns {target_columns} are in the data. Available columns are: {data.columns}\")\n",
        "\n",
        "print(f\"Columns being used for prediction: {available_columns}\")\n",
        "\n",
        "models = {}\n",
        "\n",
        "def get_model_hash():\n",
        "    model_definition = \"\"\"\n",
        "    Sequential([\n",
        "        Input(shape=(60, 1)),\n",
        "        LSTM(units=100, return_sequences=True),\n",
        "        LSTM(units=100, return_sequences=True),\n",
        "        LSTM(units=100, return_sequences=True),\n",
        "        LSTM(units=100, return_sequences=False),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    \"\"\"\n",
        "    return hashlib.md5(model_definition.encode()).hexdigest()\n",
        "\n",
        "MODEL_HASH = get_model_hash()\n",
        "\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        LSTM(units=100, return_sequences=True),\n",
        "        LSTM(units=100, return_sequences=True),\n",
        "        LSTM(units=100, return_sequences=True),\n",
        "        LSTM(units=100, return_sequences=False),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def prepare_data(data, column, scaler):\n",
        "    scaled_data = scaler.fit_transform(data[[column]].values)\n",
        "    X, y = [], []\n",
        "    for i in range(60, len(scaled_data)):\n",
        "        X.append(scaled_data[i-60:i])\n",
        "        y.append(scaled_data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def predict_next_30_days(column):\n",
        "    print(f\"Processing {column}\")\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    X, y = prepare_data(data, column, scaler)\n",
        "\n",
        "    model = create_lstm_model((60, 1))\n",
        "    weights_file = f\"{column}.weights.h5\"\n",
        "    hash_file = f\"{column}.hash\"\n",
        "\n",
        "    if os.path.exists(weights_file) and os.path.exists(hash_file):\n",
        "        with open(hash_file, 'r') as f:\n",
        "            saved_hash = f.read().strip()\n",
        "        if saved_hash == MODEL_HASH:\n",
        "            model.load_weights(weights_file)\n",
        "        else:\n",
        "            print(f\"Model changed for {column}. Retraining...\")\n",
        "            model.fit(X, y, epochs=50, batch_size=32, verbose=0)\n",
        "            model.save_weights(weights_file)\n",
        "            with open(hash_file, 'w') as f:\n",
        "                f.write(MODEL_HASH)\n",
        "    else:\n",
        "        print(f\"No saved weights for {column}. Training...\")\n",
        "        model.fit(X, y, epochs=50, batch_size=32, verbose=0)\n",
        "        model.save_weights(weights_file)\n",
        "        with open(hash_file, 'w') as f:\n",
        "            f.write(MODEL_HASH)\n",
        "\n",
        "    last_60_days = scaler.transform(data[[column]].tail(60).values)\n",
        "    current_batch = last_60_days.reshape((1, 60, 1))\n",
        "\n",
        "    predictions = []\n",
        "    for _ in range(30):\n",
        "        current_pred = model.predict(current_batch, verbose=0)\n",
        "        predictions.append(current_pred[0, 0])\n",
        "        current_batch = np.roll(current_batch, -1, axis=1)\n",
        "        current_batch[0, -1, 0] = current_pred[0, 0]\n",
        "\n",
        "    return scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
        "\n",
        "def predict_next_30_days_adjusted():\n",
        "    predictions = {column: predict_next_30_days(column) for column in available_columns}\n",
        "\n",
        "    adjusted_predictions = []\n",
        "    for i in range(30):\n",
        "        values = [predictions[col][i] for col in available_columns]\n",
        "        high = max(values)\n",
        "        low = min(values)\n",
        "        close = np.mean(values)  # or use 'Close' if available\n",
        "        if 'Close' in predictions:\n",
        "            close = np.clip(predictions['Close'][i], low, high)\n",
        "        adjusted_predictions.append((high, low, close))\n",
        "\n",
        "    return np.array(adjusted_predictions).T\n",
        "\n",
        "print(\"\\nStarting predictions:\")\n",
        "prediction_results = predict_next_30_days_adjusted()\n",
        "\n",
        "next_30_days_high, next_30_days_low, next_30_days_close = prediction_results\n",
        "\n",
        "print(f\"Predictions for Close: {next_30_days_close}\")\n",
        "print(f\"Predictions for High: {next_30_days_high}\")\n",
        "print(f\"Predictions for Low: {next_30_days_low}\")\n",
        "\n",
        "next_30_days_dates = pd.date_range(start=data['Date'].max() + timedelta(days=1), periods=30)\n",
        "next_30_days_df = pd.DataFrame({\n",
        "    'Date': next_30_days_dates,\n",
        "    'High': next_30_days_high,\n",
        "    'Low': next_30_days_low,\n",
        "    'Close': next_30_days_close,\n",
        "})\n",
        "\n",
        "# Add any missing columns as NaN\n",
        "for col in ['Open', 'Volume']:\n",
        "    if col not in next_30_days_df.columns:\n",
        "        next_30_days_df[col] = np.nan\n",
        "\n",
        "print(next_30_days_df)\n",
        "next_30_days_df.to_csv('/home/predictions.csv', index=False)\n",
        "\n",
        "# Save dates and individual predictions\n",
        "pd.DataFrame({'Date': next_30_days_dates}).to_csv('/home/predictions_dates.csv', index=False)\n",
        "pd.DataFrame({col: next_30_days_df[col] for col in ['Close', 'High', 'Low'] if col in next_30_days_df.columns}).to_csv('/home/predictions_prices.csv', index=False)"
      ]
    }
  ]
}